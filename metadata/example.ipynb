{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Generation of Intrinio based Financial Data - Shankar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/NatarajanShankar/UC_Berkeley/Final_term/Capstone/W210/MIDS_capstone/metadata\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from api_key import INTRINIO_USERNAME\n",
    "from api_key import INTRINIO_PASSWORD             \n",
    "\n",
    "import md_intrinio_client\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from md_intrinio_client import intrinio_get_company_metadata\n",
    "from md_intrinio_client import intrinio_get_company_financials\n",
    "from md_intrinio_client import intrinio_get_company_financials_csv\n",
    "from md_intrinio_client import get_SandP_metadata\n",
    "from md_intrinio_client import test_SandP_metadata\n",
    "from finsymbols import symbols\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import finsymbols\n",
    "import ast\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json \n",
    "SandP500 = {}\n",
    "companyList = []\n",
    "with open(\"SandP500_symbols.txt\", \"r\") as fr:\n",
    "            for line in fr:\n",
    "                    company = json.loads(line)\n",
    "                    SandP500[company[\"symbol\"]] = line\n",
    "                    companyList.append(company[\"symbol\"])\n",
    "\n",
    "tickerchunks = [companyList[x:x+90] for x in xrange(0, len(companyList), 90)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(tickerchunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "[u'K', u'KEY', u'KMB', u'KIM', u'KMI', u'KLAC', u'KSS', u'KHC', u'KR', u'LB', u'LLL', u'LH', u'LRCX', u'LEG', u'LEN', u'LVLT', u'LUK', u'LLY', u'LNC', u'LKQ', u'LMT', u'L', u'LOW', u'LYB', u'MTB', u'MAC', u'M', u'MRO', u'MPC', u'MAR', u'MMC', u'MLM', u'MAS', u'MA', u'MAT', u'MKC', u'MCD', u'MCK', u'MDT', u'MRK', u'MET', u'MTD', u'MGM', u'KORS', u'MCHP', u'MU', u'MSFT', u'MAA', u'MHK', u'TAP', u'MDLZ', u'MON', u'MNST', u'MCO', u'MS', u'MOS', u'MSI', u'MYL', u'NDAQ', u'NOV', u'NAVI', u'NTAP', u'NFLX', u'NWL', u'NFX', u'NEM', u'NWSA', u'NWS', u'NEE', u'NLSN', u'NKE', u'NI', u'NBL', u'JWN', u'NSC', u'NTRS', u'NOC', u'NRG', u'NUE', u'NVDA', u'ORLY', u'OXY', u'OMC', u'OKE', u'ORCL', u'PCAR', u'PKG', u'PH', u'PDCO', u'PAYX']\n"
     ]
    }
   ],
   "source": [
    "print(len(tickerchunks[3]))\n",
    "print((tickerchunks[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_intrinio_get_company_financials(symbol, year, quarter):\n",
    "        # Get the latest FY Income Statement for \"symbol\"\n",
    "        # 'type': 'FY'\n",
    "        cleanedupdata = {}\n",
    "        base_url = \"https://api.intrinio.com\"\n",
    "        request_url = base_url + \"/financials/standardized\"\n",
    "        query_params = {\n",
    "                'ticker': symbol,\n",
    "                'statement': 'income_statement',\n",
    "                'fiscal_year' : str(year),\n",
    "                'fiscal_period' : quarter\n",
    "        }\n",
    "\n",
    "        response = requests.get(request_url, params=query_params, auth=(INTRINIO_USERNAME, INTRINIO_PASSWORD))\n",
    "        if response.status_code == 401: print(\"Unauthorized! Check your username and password.\"); exit()\n",
    "\n",
    "        if response.status_code == 429:\n",
    "            print(\"API query limit reached\")\n",
    "            return\n",
    "        data = response.json()['data']\n",
    "\n",
    "\n",
    "        for row in data:\n",
    "                tag = row['tag']\n",
    "                value = row['value']\n",
    "                cleanedupdata[\"AASYMBOL\"] = symbol\n",
    "                cleanedupdata[\"ABYEAR\"] = year\n",
    "                cleanedupdata[\"ACPeriod\"] = quarter\n",
    "                cleanedupdata[tag] = value\n",
    "\n",
    "        datalist=[]\n",
    "        for key, value in sorted(cleanedupdata.items()):\n",
    "            datalist.append(str(value))\n",
    "        \n",
    "\n",
    "        return(datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updated_cleanupdata(cleanedupdata):\n",
    "    newData = {}\n",
    "    newData['AASYMBOL'] = cleanedupdata['AASYMBOL']\n",
    "    newData['ABYEAR'] = cleanedupdata['ABYEAR']\n",
    "    newData['ACPeriod'] = cleanedupdata['ACPeriod']\n",
    "    newData['basicdilutedeps'] = cleanedupdata.get('basicdilutedeps', 0.0)\n",
    "    newData['basiceps'] = cleanedupdata.get('basiceps', 0.0)\n",
    "    #print(\"basiceps is {}\".format(newData['basiceps']))\n",
    "    \n",
    "    newData['cashdividendspershare'] = cleanedupdata.get('cashdividendspershare', 0.0)\n",
    "    newData['dilutedeps'] = cleanedupdata.get('dilutedeps', 0.0)\n",
    "    newData['incometaxexpense'] = cleanedupdata.get('incometaxexpense', 0.0)\n",
    "    newData['netincome'] = cleanedupdata.get('netincome', 0.0)\n",
    "    #print(\"netincome is {}\".format(newData['netincome']))\n",
    "    newData['netincomecontinuing'] = cleanedupdata.get('netincomecontinuing', 0.0)\n",
    "    \n",
    "    newData['netincomediscontinued'] = cleanedupdata.get('netincomediscontinued', 0.0)\n",
    "    newData['netincometocommon'] = cleanedupdata.get('netincometocommon', 0.0)\n",
    "    newData['netincometononcontrollinginterest'] = cleanedupdata.get('netincometononcontrollinginterest', 0.0)\n",
    "    newData['operatingcostofrevenue'] = cleanedupdata.get('operatingcostofrevenue', 0.0)\n",
    "    newData['operatingrevenue'] = cleanedupdata.get('operatingrevenue', 0.0)\n",
    "    #print(\"operatingrevenue is {}\".format(newData['operatingrevenue']))\n",
    "    \n",
    "    newData['othercostofrevenue'] = cleanedupdata.get('othercostofrevenue', 0.0)\n",
    "    newData['otherincome'] = cleanedupdata.get('otherincome', 0.0)\n",
    "    newData['preferreddividends'] = cleanedupdata.get('preferreddividends', 0.0)\n",
    "    newData['sgaexpense'] = cleanedupdata.get('sgaexpense', 0.0)\n",
    "    newData['totalcostofrevenue'] = cleanedupdata.get('totalcostofrevenue', 0.0)\n",
    "    \n",
    "    newData['totalgrossprofit'] = cleanedupdata.get('totalgrossprofit', 0.0)\n",
    "    #print(\"totalgrossprofit is {}\".format(newData['totalgrossprofit']))\n",
    "    newData['totalinterestexpense'] = cleanedupdata.get('totalinterestexpense', 0.0)\n",
    "    newData['totaloperatingexpenses'] = cleanedupdata.get('totaloperatingexpenses', 0.0)\n",
    "    newData['totaloperatingincome'] = cleanedupdata.get('totaloperatingincome', 0.0)\n",
    "    newData['totalotherincome'] = cleanedupdata.get('totalotherincome', 0.0)\n",
    "    \n",
    "    newData['totalpretaxincome'] = cleanedupdata.get('totalpretaxincome', 0.0)\n",
    "    newData['totalrevenue'] = cleanedupdata.get('totalrevenue', 0.0)\n",
    "    #print(\"totalrevenue is {}\".format(newData['totalrevenue']))\n",
    "    newData['weightedavebasicdilutedsharesos'] = cleanedupdata.get('weightedavebasicdilutedsharesos', 0.0)\n",
    "    newData['weightedavebasicsharesos'] = cleanedupdata.get('weightedavebasicsharesos', 0.0)\n",
    "    newData['weightedavedilutedsharesos'] = cleanedupdata.get('weightedavedilutedsharesos', 0.0)\n",
    "    \n",
    "    #newData[] = cleanedupdata.get(, 0.0)\n",
    "    return newData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "ticker,year,quarter,basicdilutedeps,basiceps,cashdividendspershare,dilutedeps,incometaxexpense,netincome,netincomecontinuing,netincomediscontinued,netincometocommon,netincometononcontrollinginterest,operatingcostofrevenue,operatingrevenue,othercostofrevenue,otherincome,preferreddividends,sgaexpense,totalcostofrevenue,totalgrossprofit,totalinterestexpense,totaloperatingexpenses,totaloperatingincome,totalotherincome,totalpretaxincome,totalrevenue,weightedavebasicdilutedsharesos,weightedavebasicsharesos,weightedavedilutedsharesos\n"
     ]
    }
   ],
   "source": [
    "attributes = [\"ticker\", \"year\", \"quarter\", \"basicdilutedeps\", \"basiceps\",\n",
    "              'cashdividendspershare', 'dilutedeps', 'incometaxexpense', 'netincome', 'netincomecontinuing',\n",
    "              'netincomediscontinued', 'netincometocommon', 'netincometononcontrollinginterest',  \n",
    "                  'operatingcostofrevenue', 'operatingrevenue',\n",
    "              'othercostofrevenue', 'otherincome', 'preferreddividends', 'sgaexpense', 'totalcostofrevenue',\n",
    "              'totalgrossprofit', 'totalinterestexpense', 'totaloperatingexpenses', 'totaloperatingincome', 'totalotherincome', \n",
    "              'totalpretaxincome', 'totalrevenue', 'weightedavebasicdilutedsharesos', 'weightedavebasicsharesos', 'weightedavedilutedsharesos'\n",
    "                 ]\n",
    "print(len(attributes))\n",
    "\n",
    "xx = \",\".join(attributes)\n",
    "print(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def updated_intrinio_get_company_financials(symbol, year, quarter):\n",
    "        # Get the latest FY Income Statement for \"symbol\"\n",
    "        # 'type': 'FY'\n",
    "        cleanedupdata = {}\n",
    "        base_url = \"https://api.intrinio.com\"\n",
    "        request_url = base_url + \"/financials/standardized\"\n",
    "        query_params = {\n",
    "                'ticker': symbol,\n",
    "                'statement': 'income_statement',\n",
    "                'fiscal_year' : str(year),\n",
    "                'fiscal_period' : quarter\n",
    "        }\n",
    "\n",
    "        response = requests.get(request_url, params=query_params, auth=(INTRINIO_USERNAME, INTRINIO_PASSWORD))\n",
    "        if response.status_code == 401: print(\"Unauthorized! Check your username and password.\"); exit()\n",
    "\n",
    "        if response.status_code == 429:\n",
    "            print(\"API query limit reached\")\n",
    "            return\n",
    "        data = response.json()['data']\n",
    "\n",
    "        #print(data['basicdilutedeps'])\n",
    "        for row in data:\n",
    "                #print(row)\n",
    "                tag = row['tag']\n",
    "                value = row['value']\n",
    "\n",
    "                cleanedupdata[tag] = value\n",
    "\n",
    "        datalist=[]\n",
    "        attr = []\n",
    "        cleanedupdata[\"AASYMBOL\"] = symbol\n",
    "        cleanedupdata[\"ABYEAR\"] = year\n",
    "        cleanedupdata[\"ACPeriod\"] = quarter\n",
    "        cleanedupdata = updated_cleanupdata(cleanedupdata)\n",
    "        for key, value in sorted(cleanedupdata.items()):\n",
    "            datalist.append(str(value))\n",
    "            attr.append(str(key))\n",
    " \n",
    "        return(datalist, attr)\n",
    "data = updated_intrinio_get_company_financials('GE', '2008', 'Q1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/NatarajanShankar/UC_Berkeley/Final_term/Capstone/W210/MIDS_capstone/metadata\n",
      "/Users/NatarajanShankar/UC_Berkeley/Final_term/Capstone/W210/MIDS_capstone/data/nlp_by_company\n",
      "['COL.csv', 'CRM.csv', 'DGX.csv', 'FOX.csv', 'FOXA.csv', 'FTI.csv', 'JWN.csv', 'KORS.csv', 'LUV.csv', 'M.csv', 'MA.csv', 'MAA.csv', 'MAC.csv', 'MAR.csv', 'MAS.csv', 'MAT.csv', 'MCD.csv', 'MCHP.csv', 'MCK.csv', 'MCO.csv', 'MDLZ.csv', 'MDT.csv', 'MET.csv', 'MGM.csv', 'MHK.csv', 'MKC.csv', 'MLM.csv', 'MMC.csv', 'MNST.csv', 'MON.csv', 'MOS.csv', 'MPC.csv', 'MRK.csv', 'MRO.csv', 'MS.csv', 'MSFT.csv', 'MSI.csv', 'MTD.csv', 'MU.csv', 'MYL.csv', 'NAVI.csv', 'NDAQ.csv', 'NEE.csv', 'NEM.csv', 'NFLX.csv', 'NFX.csv', 'NI.csv', 'NKE.csv', 'NLSN.csv', 'NOC.csv', 'NOV.csv', 'NRG.csv', 'NSC.csv', 'NTAP.csv', 'NTRS.csv', 'NUE.csv', 'NVDA.csv', 'NWS.csv', 'NWSA.csv', 'O.csv', 'OKE.csv', 'OMC.csv', 'ORCL.csv', 'ORLY.csv', 'OXY.csv', 'PAYX.csv', 'PBCT.csv', 'PCAR.csv', 'PCG.csv', 'PCLN.csv', 'PDCO.csv', 'PEG.csv', 'PEP.csv', 'PFE.csv', 'PFG.csv', 'PG.csv', 'PGR.csv', 'PH.csv', 'PHM.csv', 'PKG.csv', 'PKI.csv', 'PLD.csv', 'PM.csv', 'PNC.csv', 'PNR.csv', 'PNW.csv', 'PPG.csv', 'PPL.csv', 'PRGO.csv', 'PRU.csv', 'PSA.csv', 'PSX.csv', 'PVH.csv', 'PWR.csv', 'PX.csv', 'PXD.csv', 'PYPL.csv', 'Q.csv', 'QCOM.csv', 'QRVO.csv', 'RCL.csv', 'REG.csv', 'REGN.csv', 'RF.csv', 'RHI.csv', 'RHT.csv', 'RJF.csv', 'RL.csv', 'RMD.csv', 'ROK.csv', 'ROP.csv', 'ROST.csv', 'RRC.csv', 'RSG.csv', 'RTN.csv', 'SBAC.csv', 'SBUX.csv', 'SCG.csv', 'SEE.csv', 'SHW.csv', 'SIG.csv', 'SLB.csv', 'SLG.csv', 'SNA.csv', 'SNI.csv', 'SNPS.csv', 'SO.csv', 'SPG.csv', 'SPGI.csv', 'SRCL.csv', 'SRE.csv', 'STI.csv', 'STT.csv', 'STX.csv', 'SWK.csv', 'SWKS.csv', 'SYF.csv', 'SYK.csv', 'SYMC.csv', 'SYY.csv', 'TAP.csv', 'TDG.csv', 'TEL.csv', 'TGT.csv', 'TIF.csv', 'TJX.csv', 'TMK.csv', 'TMO.csv', 'TRIP.csv', 'TROW.csv', 'TRV.csv', 'TSCO.csv', 'TSN.csv', 'TSS.csv', 'TWX.csv', 'TXN.csv', 'TXT.csv', 'UDR.csv', 'ULTA.csv', 'USB.csv']\n",
      "['COL', 'CRM', 'DGX', 'FOX', 'FOXA', 'FTI', 'JWN', 'KORS', 'LUV', 'M', 'MA', 'MAA', 'MAC', 'MAR', 'MAS', 'MAT', 'MCD', 'MCHP', 'MCK', 'MCO', 'MDLZ', 'MDT', 'MET', 'MGM', 'MHK', 'MKC', 'MLM', 'MMC', 'MNST', 'MON', 'MOS', 'MPC', 'MRK', 'MRO', 'MS', 'MSFT', 'MSI', 'MTD', 'MU', 'MYL', 'NAVI', 'NDAQ', 'NEE', 'NEM', 'NFLX', 'NFX', 'NI', 'NKE', 'NLSN', 'NOC', 'NOV', 'NRG', 'NSC', 'NTAP', 'NTRS', 'NUE', 'NVDA', 'NWS', 'NWSA', 'O', 'OKE', 'OMC', 'ORCL', 'ORLY', 'OXY', 'PAYX', 'PBCT', 'PCAR', 'PCG', 'PCLN', 'PDCO', 'PEG', 'PEP', 'PFE', 'PFG', 'PG', 'PGR', 'PH', 'PHM', 'PKG', 'PKI', 'PLD', 'PM', 'PNC', 'PNR', 'PNW', 'PPG', 'PPL', 'PRGO', 'PRU', 'PSA', 'PSX', 'PVH', 'PWR', 'PX', 'PXD', 'PYPL', 'Q', 'QCOM', 'QRVO', 'RCL', 'REG', 'REGN', 'RF', 'RHI', 'RHT', 'RJF', 'RL', 'RMD', 'ROK', 'ROP', 'ROST', 'RRC', 'RSG', 'RTN', 'SBAC', 'SBUX', 'SCG', 'SEE', 'SHW', 'SIG', 'SLB', 'SLG', 'SNA', 'SNI', 'SNPS', 'SO', 'SPG', 'SPGI', 'SRCL', 'SRE', 'STI', 'STT', 'STX', 'SWK', 'SWKS', 'SYF', 'SYK', 'SYMC', 'SYY', 'TAP', 'TDG', 'TEL', 'TGT', 'TIF', 'TJX', 'TMK', 'TMO', 'TRIP', 'TROW', 'TRV', 'TSCO', 'TSN', 'TSS', 'TWX', 'TXN', 'TXT', 'UDR', 'ULTA', 'USB']\n",
      "/Users/NatarajanShankar/UC_Berkeley/Final_term/Capstone/W210/MIDS_capstone/metadata\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "%cd ../data/nlp_by_company\n",
    "import glob\n",
    "xx = list(glob.glob(\"*.csv\"))\n",
    "print(xx)\n",
    "yy = []\n",
    "\n",
    "for item in xx:\n",
    "    yy.append(str(item).strip('.csv'))\n",
    "\n",
    "print(yy)\n",
    "%cd ../../metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n"
     ]
    }
   ],
   "source": [
    "company = ['COL', 'CRM', 'DGX', 'FOX', 'FOXA', 'FTI', 'JWN', 'KORS', 'LUV', 'M', 'MA', 'MAA', \n",
    "               'MAC', 'MAR', 'MAS', 'MAT', 'MCD', 'MCHP', 'MCK', 'MCO', 'MDLZ', 'MDT', 'MET', 'MGM', \n",
    "               'MHK', 'MKC', 'MLM', 'MMC', 'MNST', 'MON', 'MOS', 'MPC', 'MRK', 'MRO', 'MS', 'MSFT', \n",
    "               'MSI', 'MTD', 'MU', 'MYL', 'NAVI', 'NDAQ', 'NEE', 'NEM', 'NFLX', 'NFX', 'NI', 'NKE', \n",
    "               'NLSN', 'NOC', 'NOV', 'NRG', 'NSC', 'NTAP', 'NTRS', 'NUE', 'NVDA', 'NWS', 'NWSA', 'O', \n",
    "               'OKE', 'OMC', 'ORCL', 'ORLY', 'OXY', 'PAYX', 'PBCT', 'PCAR', 'PCG', 'PCLN', 'PDCO', \n",
    "               'PEG', 'PEP', 'PFE', 'PFG', 'PG', 'PGR', 'PH', 'PHM', 'PKG', 'PKI', 'PLD', 'PM', 'PNC', \n",
    "               'PNR', 'PNW', 'PPG', 'PPL', 'PRGO', 'PRU', 'PSA', 'PSX', 'PVH', 'PWR', 'PX', 'PXD', \n",
    "               'PYPL', 'Q', 'QCOM', 'QRVO', 'RCL', 'REG', 'REGN', 'RF', 'RHI', 'RHT', 'RJF', 'RL', \n",
    "               'RMD', 'ROK', 'ROP', 'ROST', 'RRC', 'RSG', 'RTN', 'SBAC', 'SBUX', 'SCG', 'SEE', 'SHW', \n",
    "               'SIG', 'SLB', 'SLG', 'SNA', 'SNI', 'SNPS', 'SO', 'SPG', 'SPGI', 'SRCL', 'SRE', 'STI', \n",
    "               'STT', 'STX', 'SWK', 'SWKS', 'SYF', 'SYK', 'SYMC', 'SYY', 'TAP', 'TDG', 'TEL', 'TGT', \n",
    "               'TIF', 'TJX', 'TMK', 'TMO', 'TRIP', 'TROW', 'TRV', 'TSCO', 'TSN', 'TSS', 'TWX', 'TXN', \n",
    "               'TXT', 'UDR', 'ULTA', 'USB']\n",
    "\n",
    "print(len(company))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_financial_data():\n",
    "    SandP500 = {}\n",
    "    companyList = []\n",
    "    with open(\"SandP500_symbols.txt\", \"r\") as fr:\n",
    "            for line in fr:\n",
    "                    company = json.loads(line)\n",
    "                    SandP500[company[\"symbol\"]] = line\n",
    "                    companyList.append(company[\"symbol\"])\n",
    "\n",
    "    tickerchunks = [companyList[x:x+95] for x in xrange(0, len(companyList), 95)]\n",
    "\n",
    "\n",
    "    nlp_companies = ['COL', 'CRM', 'DGX', 'FOX', 'FOXA', 'FTI', 'JWN', 'KORS', 'LUV', 'M', 'MA', 'MAA', \n",
    "               'MAC', 'MAR', 'MAS', 'MAT', 'MCD', 'MCHP', 'MCK', 'MCO', 'MDLZ', 'MDT', 'MET', 'MGM', \n",
    "               'MHK', 'MKC', 'MLM', 'MMC', 'MNST', 'MON', 'MOS', 'MPC', 'MRK', 'MRO', 'MS', 'MSFT', \n",
    "               'MSI', 'MTD', 'MU', 'MYL', 'NAVI', 'NDAQ', 'NEE', 'NEM', 'NFLX', 'NFX', 'NI', 'NKE', \n",
    "               'NLSN', 'NOC', 'NOV', 'NRG', 'NSC', 'NTAP', 'NTRS', 'NUE', 'NVDA', 'NWS', 'NWSA', 'O', \n",
    "               'OKE', 'OMC', 'ORCL', 'ORLY', 'OXY', 'PAYX', 'PBCT', 'PCAR', 'PCG', 'PCLN', 'PDCO', \n",
    "               'PEG', 'PEP', 'PFE', 'PFG', 'PG', 'PGR', 'PH', 'PHM', 'PKG', 'PKI', 'PLD', 'PM', 'PNC', \n",
    "               'PNR', 'PNW', 'PPG', 'PPL', 'PRGO', 'PRU', 'PSA', 'PSX', 'PVH', 'PWR', 'PX', 'PXD', \n",
    "               'PYPL', 'Q', 'QCOM', 'QRVO', 'RCL', 'REG', 'REGN', 'RF', 'RHI', 'RHT', 'RJF', 'RL', \n",
    "               'RMD', 'ROK', 'ROP', 'ROST', 'RRC', 'RSG', 'RTN', 'SBAC', 'SBUX', 'SCG', 'SEE', 'SHW', \n",
    "               'SIG', 'SLB', 'SLG', 'SNA', 'SNI', 'SNPS', 'SO', 'SPG', 'SPGI', 'SRCL', 'SRE', 'STI', \n",
    "               'STT', 'STX', 'SWK', 'SWKS', 'SYF', 'SYK', 'SYMC', 'SYY', 'TAP', 'TDG', 'TEL', 'TGT', \n",
    "               'TIF', 'TJX', 'TMK', 'TMO', 'TRIP', 'TROW', 'TRV', 'TSCO', 'TSN', 'TSS', 'TWX', 'TXN', \n",
    "               'TXT', 'UDR', 'ULTA', 'USB']\n",
    "    years = [2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017]\n",
    "    quarters = [\"Q1\", \"Q2\", \"Q3\", \"Q4\", \"FY\"]\n",
    "    #years = [2010]\n",
    "    #quarters = [\"Q1\", \"Q2\"]\n",
    "    #companies = [\"GE\", \"CSCO\", \"GOOG\", \"FACE\"]\n",
    "\n",
    "    bigDict = {}\n",
    "\n",
    "    attributes = [\"ticker\", \"year\", \"quarter\", \"basicdilutedeps\", \"basiceps\",\n",
    "              'cashdividendspershare', 'dilutedeps', 'incometaxexpense', 'netincome', 'netincomecontinuing',\n",
    "              'netincomediscontinued', 'netincometocommon', 'netincometononcontrollinginterest',  \n",
    "                  'operatingcostofrevenue', 'operatingrevenue',\n",
    "              'othercostofrevenue', 'otherincome', 'preferreddividends', 'sgaexpense', 'totalcostofrevenue',\n",
    "              'totalgrossprofit', 'totalinterestexpense', 'totaloperatingexpenses', 'totaloperatingincome', 'totalotherincome', \n",
    "              'totalpretaxincome', 'totalrevenue', 'weightedavebasicdilutedsharesos', 'weightedavebasicsharesos', 'weightedavedilutedsharesos'\n",
    "                 ]\n",
    "    \n",
    "\n",
    "    print(len(attributes))\n",
    "\n",
    "    xx = \",\".join(attributes) + \"\\n\"\n",
    "\n",
    "    # tickerchunks[1], tickerchunks[2], tickerchunks[3] and tickerchunks[5] are done\n",
    "    # Re-doing 0 block\n",
    "    loopIndex = 0\n",
    "    for company in tickerchunks[0]:\n",
    "    #for company in nlp_companies:\n",
    "        #with open(\"../data/nlp_by_company/revenue/\"+company+\"_Financials_by_Quarter.csv\", 'w') as fw:\n",
    "        with open(\"./revenue/\"+company+\"_Financials_by_Quarter.csv\", 'w') as fw:\n",
    "            fw.write(xx)\n",
    "            print(\"working on {} - {}\".format(company, loopIndex))\n",
    "#             if loopIndex > 2:\n",
    "#                 break\n",
    "            loopIndex += 1\n",
    "            #print(\"working on company {}\".format(company))\n",
    "            for year in years:\n",
    "\n",
    "                #print(\"working on year {}\".format(year))\n",
    "                for quarter in quarters:\n",
    "                    #print(\"working on quarter {}\".format(quarter))\n",
    "\n",
    "                    data, _ = updated_intrinio_get_company_financials(company, str(year), quarter)\n",
    "                    \n",
    "                    # Convert list to string\n",
    "                    datastring = \",\".join(data)\n",
    "                    \n",
    "                    # Add a linefeed so that every data point is on a different line\n",
    "                    datastring +=\"\\n\"\n",
    "\n",
    "                    # Write the data to the open file for this company\n",
    "                    fw.write(datastring)\n",
    "                    #break\n",
    "                #break\n",
    "            #break\n",
    "        #break\n",
    "\n",
    "            print(\"DONE with {}\".format(company))\n",
    "            #return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/NatarajanShankar/UC_Berkeley/Final_term/Capstone/W210/MIDS_capstone/metadata\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "working on MMM - 0\n",
      "DONE with MMM\n",
      "working on ABT - 1\n",
      "DONE with ABT\n",
      "working on ABBV - 2\n",
      "DONE with ABBV\n",
      "working on ACN - 3\n",
      "DONE with ACN\n",
      "working on ATVI - 4\n",
      "DONE with ATVI\n",
      "working on AYI - 5\n",
      "DONE with AYI\n",
      "working on ADBE - 6\n",
      "DONE with ADBE\n",
      "working on AMD - 7\n",
      "DONE with AMD\n",
      "working on AAP - 8\n",
      "DONE with AAP\n",
      "working on AES - 9\n",
      "DONE with AES\n",
      "working on AET - 10\n",
      "DONE with AET\n",
      "working on AMG - 11\n",
      "DONE with AMG\n",
      "working on AFL - 12\n",
      "DONE with AFL\n",
      "working on A - 13\n",
      "DONE with A\n",
      "working on APD - 14\n",
      "DONE with APD\n",
      "working on AKAM - 15\n",
      "DONE with AKAM\n",
      "working on ALK - 16\n",
      "DONE with ALK\n",
      "working on ALB - 17\n",
      "DONE with ALB\n",
      "working on ARE - 18\n",
      "DONE with ARE\n",
      "working on ALXN - 19\n",
      "DONE with ALXN\n",
      "working on ALGN - 20\n",
      "DONE with ALGN\n",
      "working on ALLE - 21\n",
      "DONE with ALLE\n",
      "working on AGN - 22\n",
      "DONE with AGN\n",
      "working on ADS - 23\n",
      "DONE with ADS\n",
      "working on LNT - 24\n",
      "DONE with LNT\n",
      "working on ALL - 25\n",
      "DONE with ALL\n",
      "working on GOOGL - 26\n",
      "DONE with GOOGL\n",
      "working on GOOG - 27\n",
      "DONE with GOOG\n",
      "working on MO - 28\n",
      "DONE with MO\n",
      "working on AMZN - 29\n",
      "DONE with AMZN\n",
      "working on AEE - 30\n",
      "DONE with AEE\n",
      "working on AAL - 31\n",
      "DONE with AAL\n",
      "working on AEP - 32\n",
      "DONE with AEP\n",
      "working on AXP - 33\n",
      "DONE with AXP\n",
      "working on AIG - 34\n",
      "DONE with AIG\n",
      "working on AMT - 35\n",
      "DONE with AMT\n",
      "working on AWK - 36\n",
      "DONE with AWK\n",
      "working on AMP - 37\n",
      "DONE with AMP\n",
      "working on ABC - 38\n",
      "DONE with ABC\n",
      "working on AME - 39\n",
      "DONE with AME\n",
      "working on AMGN - 40\n",
      "DONE with AMGN\n",
      "working on APH - 41\n",
      "DONE with APH\n",
      "working on APC - 42\n",
      "DONE with APC\n",
      "working on ADI - 43\n",
      "DONE with ADI\n",
      "working on ANDV - 44\n",
      "DONE with ANDV\n",
      "working on ANSS - 45\n",
      "DONE with ANSS\n",
      "working on ANTM - 46\n",
      "DONE with ANTM\n",
      "working on AON - 47\n",
      "DONE with AON\n",
      "working on AOS - 48\n",
      "DONE with AOS\n",
      "working on APA - 49\n",
      "DONE with APA\n",
      "working on AIV - 50\n",
      "DONE with AIV\n",
      "working on AAPL - 51\n",
      "DONE with AAPL\n",
      "working on AMAT - 52\n",
      "DONE with AMAT\n",
      "working on ADM - 53\n",
      "DONE with ADM\n",
      "working on ARNC - 54\n",
      "DONE with ARNC\n",
      "working on AJG - 55\n",
      "DONE with AJG\n",
      "working on AIZ - 56\n",
      "DONE with AIZ\n",
      "working on T - 57\n",
      "DONE with T\n",
      "working on ADSK - 58\n",
      "DONE with ADSK\n",
      "working on ADP - 59\n",
      "DONE with ADP\n",
      "working on AZO - 60\n",
      "DONE with AZO\n",
      "working on AVB - 61\n",
      "DONE with AVB\n",
      "working on AVY - 62\n",
      "DONE with AVY\n",
      "working on BHGE - 63\n",
      "DONE with BHGE\n",
      "working on BLL - 64\n",
      "DONE with BLL\n",
      "working on BAC - 65\n",
      "DONE with BAC\n",
      "working on BK - 66\n",
      "DONE with BK\n",
      "working on BCR - 67\n",
      "DONE with BCR\n",
      "working on BAX - 68\n",
      "DONE with BAX\n",
      "working on BBT - 69\n",
      "DONE with BBT\n",
      "working on BDX - 70\n",
      "DONE with BDX\n",
      "working on BRK.B - 71\n",
      "DONE with BRK.B\n",
      "working on BBY - 72\n",
      "DONE with BBY\n",
      "working on BIIB - 73\n",
      "DONE with BIIB\n",
      "working on BLK - 74\n",
      "DONE with BLK\n",
      "working on HRB - 75\n",
      "DONE with HRB\n",
      "working on BA - 76\n",
      "DONE with BA\n",
      "working on BWA - 77\n",
      "DONE with BWA\n",
      "working on BXP - 78\n",
      "DONE with BXP\n",
      "working on BSX - 79\n",
      "DONE with BSX\n",
      "working on BHF - 80\n",
      "DONE with BHF\n",
      "working on BMY - 81\n",
      "DONE with BMY\n",
      "working on AVGO - 82\n",
      "DONE with AVGO\n",
      "working on BF.B - 83\n",
      "DONE with BF.B\n",
      "working on CHRW - 84\n",
      "DONE with CHRW\n",
      "working on CA - 85\n",
      "DONE with CA\n",
      "working on COG - 86\n",
      "DONE with COG\n",
      "working on CDNS - 87\n",
      "DONE with CDNS\n",
      "working on CPB - 88\n",
      "DONE with CPB\n",
      "working on COF - 89\n",
      "DONE with COF\n",
      "working on CAH - 90\n",
      "DONE with CAH\n",
      "working on CBOE - 91\n",
      "DONE with CBOE\n",
      "working on KMX - 92\n",
      "DONE with KMX\n",
      "working on CCL - 93\n",
      "DONE with CCL\n",
      "working on CAT - 94\n",
      "DONE with CAT\n"
     ]
    }
   ],
   "source": [
    "generate_financial_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/NatarajanShankar/UC_Berkeley/Final_term/Capstone/W210/MIDS_capstone/metadata\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/NatarajanShankar/UC_Berkeley/Final_term/Capstone/W210/MIDS_capstone/metadata\")\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import ast\n",
    "\n",
    "def convert_json_to_csv():\n",
    "    companyDict = {}\n",
    "    os.chdir(\"./data\")\n",
    "    #print(\"got here\")\n",
    "    for datafile in glob.glob(\"*Financials*.json\"):\n",
    "        #print(datafile)\n",
    "        with open(datafile, \"r\") as fr:\n",
    "            #print (\"Processing {}\".format(datafile))\n",
    "            for line in fr:\n",
    "                companyDict = ast.literal_eval(line)\n",
    "                \n",
    "                # Save company ticker\n",
    "                for company, companyvalue in companyDict.items():\n",
    "                    #print(\"Company is\")\n",
    "                    #print(\"{}\".format(company, end=''))\n",
    "                    # For every year subset\n",
    "                    if isinstance(companyvalue, dict):\n",
    "                        for year, yearvalue in companyvalue.items():\n",
    "                            #print(\"Year is\")\n",
    "                            #print(\"{}\".format(year, end=''))\n",
    "                            # Look at the corresponding quarter\n",
    "                            if isinstance(yearvalue, dict):\n",
    "                                datalist=[]\n",
    "                                # Look at company quarter\n",
    "                                for quarter, quartervalue in yearvalue.items():\n",
    "                                    #print(\"Quarter is\")\n",
    "                                    #print(\"{}\".format(quarter, end=''))\n",
    "                                    for financialdata in quartervalue.values():\n",
    "                                        datalist.append(financialdata)\n",
    "                                    \n",
    "                                    if len(datalist) > 21:\n",
    "                                        datalist[14] = \"IGNORE\"\n",
    "                                        datalist[21] = \"IGNORE\"\n",
    "                                    \n",
    "                                    if len(datalist) == 2:\n",
    "                                        datalist[0] = \"\"\n",
    "                                        datalist[1] = \"\"\n",
    "                                    print(\"{},{},{},{}\".format(company, year, quarter, \",\".join(datalist), end=''))\n",
    "                                    \n",
    "                                    datalist = []\n",
    "        break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLIR,2010,Q1,\n",
      "FLIR,2010,Q4,\n"
     ]
    }
   ],
   "source": [
    "convert_json_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_company_financial_data_all(symbol):\n",
    "    companyDict = {}\n",
    "    with open(\"./data/Financials.json\", \"r\") as fp:\n",
    "        #companyDict = json.load(fp)\n",
    "        for line in fp:\n",
    "            companyDict = ast.literal_eval(line)\n",
    "\n",
    "        for ticker in companyDict.keys():\n",
    "            if ticker == symbol:\n",
    "                print symbol, companyDict[symbol]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_company_financial_data_all(\"GE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_company_financial_data_by_year(symbol, year):\n",
    "    companyDict = {}\n",
    "    with open(\"./data/Financials.json\", \"r\") as fp:\n",
    "        #companyDict = json.load(fp)\n",
    "        for line in fp:\n",
    "            companyDict = ast.literal_eval(line)\n",
    "\n",
    "        for ticker in companyDict.keys():\n",
    "            if ticker == symbol:\n",
    "                print symbol, companyDict[symbol][year]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_company_financial_data_by_year(\"GE\", \"2014\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_company_financial_data_by_year_and_attr(symbol, year, attribute):\n",
    "    companyDict = {}\n",
    "    with open(\"./data/Financials.json\", \"r\") as fp:\n",
    "        #companyDict = json.load(fp)\n",
    "        for line in fp:\n",
    "            companyDict = ast.literal_eval(line)\n",
    "\n",
    "        for ticker in companyDict.keys():\n",
    "            if ticker == symbol:\n",
    "                print symbol, companyDict[symbol][year][attribute]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_company_financial_data_by_year_and_attr(\"GE\", \"2014\", \"basiceps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_company_financial_data_csv(symbol):\n",
    "    companyDict = {}\n",
    "    with open(\"./data/Financials.json\", \"r\") as fp:\n",
    "        for line in fp:\n",
    "            companyDict = ast.literal_eval(line)\n",
    "            \n",
    "        for ticker in companyDict.keys():\n",
    "            if ticker == symbol:\n",
    "                print symbol, companyDict\n",
    "            \n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
