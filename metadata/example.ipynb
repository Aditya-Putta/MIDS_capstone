{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Generation of Intrinio based Financial Data - Shankar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/NatarajanShankar/UC_Berkeley/Final_term/Capstone/W210/MIDS_capstone/metadata\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from api_key import INTRINIO_USERNAME\n",
    "from api_key import INTRINIO_PASSWORD             \n",
    "\n",
    "import md_intrinio_client\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from md_intrinio_client import intrinio_get_company_metadata\n",
    "from md_intrinio_client import intrinio_get_company_financials\n",
    "from md_intrinio_client import intrinio_get_company_financials_csv\n",
    "from md_intrinio_client import get_SandP_metadata\n",
    "from md_intrinio_client import test_SandP_metadata\n",
    "from finsymbols import symbols\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import finsymbols\n",
    "import ast\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json \n",
    "SandP500 = {}\n",
    "companyList = []\n",
    "with open(\"SandP500_symbols.txt\", \"r\") as fr:\n",
    "            for line in fr:\n",
    "                    company = json.loads(line)\n",
    "                    SandP500[company[\"symbol\"]] = line\n",
    "                    companyList.append(company[\"symbol\"])\n",
    "\n",
    "tickerchunks = [companyList[x:x+90] for x in xrange(0, len(companyList), 90)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(tickerchunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "[u'K', u'KEY', u'KMB', u'KIM', u'KMI', u'KLAC', u'KSS', u'KHC', u'KR', u'LB', u'LLL', u'LH', u'LRCX', u'LEG', u'LEN', u'LVLT', u'LUK', u'LLY', u'LNC', u'LKQ', u'LMT', u'L', u'LOW', u'LYB', u'MTB', u'MAC', u'M', u'MRO', u'MPC', u'MAR', u'MMC', u'MLM', u'MAS', u'MA', u'MAT', u'MKC', u'MCD', u'MCK', u'MDT', u'MRK', u'MET', u'MTD', u'MGM', u'KORS', u'MCHP', u'MU', u'MSFT', u'MAA', u'MHK', u'TAP', u'MDLZ', u'MON', u'MNST', u'MCO', u'MS', u'MOS', u'MSI', u'MYL', u'NDAQ', u'NOV', u'NAVI', u'NTAP', u'NFLX', u'NWL', u'NFX', u'NEM', u'NWSA', u'NWS', u'NEE', u'NLSN', u'NKE', u'NI', u'NBL', u'JWN', u'NSC', u'NTRS', u'NOC', u'NRG', u'NUE', u'NVDA', u'ORLY', u'OXY', u'OMC', u'OKE', u'ORCL', u'PCAR', u'PKG', u'PH', u'PDCO', u'PAYX']\n"
     ]
    }
   ],
   "source": [
    "print(len(tickerchunks[3]))\n",
    "print((tickerchunks[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_intrinio_get_company_financials(symbol, year, quarter):\n",
    "        # Get the latest FY Income Statement for \"symbol\"\n",
    "        # 'type': 'FY'\n",
    "        cleanedupdata = {}\n",
    "        base_url = \"https://api.intrinio.com\"\n",
    "        request_url = base_url + \"/financials/standardized\"\n",
    "        query_params = {\n",
    "                'ticker': symbol,\n",
    "                'statement': 'income_statement',\n",
    "                'fiscal_year' : str(year),\n",
    "                'fiscal_period' : quarter\n",
    "        }\n",
    "\n",
    "        response = requests.get(request_url, params=query_params, auth=(INTRINIO_USERNAME, INTRINIO_PASSWORD))\n",
    "        if response.status_code == 401: print(\"Unauthorized! Check your username and password.\"); exit()\n",
    "\n",
    "        if response.status_code == 429:\n",
    "            print(\"API query limit reached\")\n",
    "            return\n",
    "        data = response.json()['data']\n",
    "\n",
    "\n",
    "        for row in data:\n",
    "                tag = row['tag']\n",
    "                value = row['value']\n",
    "                cleanedupdata[\"AASYMBOL\"] = symbol\n",
    "                cleanedupdata[\"ABYEAR\"] = year\n",
    "                cleanedupdata[\"ACPeriod\"] = quarter\n",
    "                cleanedupdata[tag] = value\n",
    "\n",
    "        datalist=[]\n",
    "        for key, value in sorted(cleanedupdata.items()):\n",
    "            datalist.append(str(value))\n",
    "        \n",
    "\n",
    "        return(datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updated_cleanupdata(cleanedupdata):\n",
    "    newData = {}\n",
    "    newData['AASYMBOL'] = cleanedupdata['AASYMBOL']\n",
    "    newData['ABYEAR'] = cleanedupdata['ABYEAR']\n",
    "    newData['ACPeriod'] = cleanedupdata['ACPeriod']\n",
    "    newData['basicdilutedeps'] = cleanedupdata.get('basicdilutedeps', 0.0)\n",
    "    newData['basiceps'] = cleanedupdata.get('basiceps', 0.0)\n",
    "    #print(\"basiceps is {}\".format(newData['basiceps']))\n",
    "    \n",
    "    newData['cashdividendspershare'] = cleanedupdata.get('cashdividendspershare', 0.0)\n",
    "    newData['dilutedeps'] = cleanedupdata.get('dilutedeps', 0.0)\n",
    "    newData['incometaxexpense'] = cleanedupdata.get('incometaxexpense', 0.0)\n",
    "    newData['netincome'] = cleanedupdata.get('netincome', 0.0)\n",
    "    #print(\"netincome is {}\".format(newData['netincome']))\n",
    "    newData['netincomecontinuing'] = cleanedupdata.get('netincomecontinuing', 0.0)\n",
    "    \n",
    "    newData['netincomediscontinued'] = cleanedupdata.get('netincomediscontinued', 0.0)\n",
    "    newData['netincometocommon'] = cleanedupdata.get('netincometocommon', 0.0)\n",
    "    newData['netincometononcontrollinginterest'] = cleanedupdata.get('netincometononcontrollinginterest', 0.0)\n",
    "    newData['operatingcostofrevenue'] = cleanedupdata.get('operatingcostofrevenue', 0.0)\n",
    "    newData['operatingrevenue'] = cleanedupdata.get('operatingrevenue', 0.0)\n",
    "    #print(\"operatingrevenue is {}\".format(newData['operatingrevenue']))\n",
    "    \n",
    "    newData['othercostofrevenue'] = cleanedupdata.get('othercostofrevenue', 0.0)\n",
    "    newData['otherincome'] = cleanedupdata.get('otherincome', 0.0)\n",
    "    newData['preferreddividends'] = cleanedupdata.get('preferreddividends', 0.0)\n",
    "    newData['sgaexpense'] = cleanedupdata.get('sgaexpense', 0.0)\n",
    "    newData['totalcostofrevenue'] = cleanedupdata.get('totalcostofrevenue', 0.0)\n",
    "    \n",
    "    newData['totalgrossprofit'] = cleanedupdata.get('totalgrossprofit', 0.0)\n",
    "    #print(\"totalgrossprofit is {}\".format(newData['totalgrossprofit']))\n",
    "    newData['totalinterestexpense'] = cleanedupdata.get('totalinterestexpense', 0.0)\n",
    "    newData['totaloperatingexpenses'] = cleanedupdata.get('totaloperatingexpenses', 0.0)\n",
    "    newData['totaloperatingincome'] = cleanedupdata.get('totaloperatingincome', 0.0)\n",
    "    newData['totalotherincome'] = cleanedupdata.get('totalotherincome', 0.0)\n",
    "    \n",
    "    newData['totalpretaxincome'] = cleanedupdata.get('totalpretaxincome', 0.0)\n",
    "    newData['totalrevenue'] = cleanedupdata.get('totalrevenue', 0.0)\n",
    "    #print(\"totalrevenue is {}\".format(newData['totalrevenue']))\n",
    "    newData['weightedavebasicdilutedsharesos'] = cleanedupdata.get('weightedavebasicdilutedsharesos', 0.0)\n",
    "    newData['weightedavebasicsharesos'] = cleanedupdata.get('weightedavebasicsharesos', 0.0)\n",
    "    newData['weightedavedilutedsharesos'] = cleanedupdata.get('weightedavedilutedsharesos', 0.0)\n",
    "    \n",
    "    #newData[] = cleanedupdata.get(, 0.0)\n",
    "    return newData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "ticker,year,quarter,basicdilutedeps,basiceps,cashdividendspershare,dilutedeps,incometaxexpense,netincome,netincomecontinuing,netincomediscontinued,netincometocommon,netincometononcontrollinginterest,operatingcostofrevenue,operatingrevenue,othercostofrevenue,otherincome,preferreddividends,sgaexpense,totalcostofrevenue,totalgrossprofit,totalinterestexpense,totaloperatingexpenses,totaloperatingincome,totalotherincome,totalpretaxincome,totalrevenue,weightedavebasicdilutedsharesos,weightedavebasicsharesos,weightedavedilutedsharesos\n"
     ]
    }
   ],
   "source": [
    "attributes = [\"ticker\", \"year\", \"quarter\", \"basicdilutedeps\", \"basiceps\",\n",
    "              'cashdividendspershare', 'dilutedeps', 'incometaxexpense', 'netincome', 'netincomecontinuing',\n",
    "              'netincomediscontinued', 'netincometocommon', 'netincometononcontrollinginterest',  \n",
    "                  'operatingcostofrevenue', 'operatingrevenue',\n",
    "              'othercostofrevenue', 'otherincome', 'preferreddividends', 'sgaexpense', 'totalcostofrevenue',\n",
    "              'totalgrossprofit', 'totalinterestexpense', 'totaloperatingexpenses', 'totaloperatingincome', 'totalotherincome', \n",
    "              'totalpretaxincome', 'totalrevenue', 'weightedavebasicdilutedsharesos', 'weightedavebasicsharesos', 'weightedavedilutedsharesos'\n",
    "                 ]\n",
    "print(len(attributes))\n",
    "\n",
    "xx = \",\".join(attributes)\n",
    "print(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def updated_intrinio_get_company_financials(symbol, year, quarter):\n",
    "        # Get the latest FY Income Statement for \"symbol\"\n",
    "        # 'type': 'FY'\n",
    "        cleanedupdata = {}\n",
    "        base_url = \"https://api.intrinio.com\"\n",
    "        request_url = base_url + \"/financials/standardized\"\n",
    "        query_params = {\n",
    "                'ticker': symbol,\n",
    "                'statement': 'income_statement',\n",
    "                'fiscal_year' : str(year),\n",
    "                'fiscal_period' : quarter\n",
    "        }\n",
    "\n",
    "        response = requests.get(request_url, params=query_params, auth=(INTRINIO_USERNAME, INTRINIO_PASSWORD))\n",
    "        if response.status_code == 401: print(\"Unauthorized! Check your username and password.\"); exit()\n",
    "\n",
    "        if response.status_code == 429:\n",
    "            print(\"API query limit reached\")\n",
    "            return\n",
    "        data = response.json()['data']\n",
    "\n",
    "        #print(data['basicdilutedeps'])\n",
    "        for row in data:\n",
    "                #print(row)\n",
    "                tag = row['tag']\n",
    "                value = row['value']\n",
    "\n",
    "                cleanedupdata[tag] = value\n",
    "\n",
    "        datalist=[]\n",
    "        attr = []\n",
    "        cleanedupdata[\"AASYMBOL\"] = symbol\n",
    "        cleanedupdata[\"ABYEAR\"] = year\n",
    "        cleanedupdata[\"ACPeriod\"] = quarter\n",
    "        cleanedupdata = updated_cleanupdata(cleanedupdata)\n",
    "        for key, value in sorted(cleanedupdata.items()):\n",
    "            datalist.append(str(value))\n",
    "            attr.append(str(key))\n",
    " \n",
    "        return(datalist, attr)\n",
    "data = updated_intrinio_get_company_financials('GE', '2008', 'Q1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/NatarajanShankar/UC_Berkeley/Final_term/Capstone/W210/MIDS_capstone/metadata\n",
      "/Users/NatarajanShankar/UC_Berkeley/Final_term/Capstone/W210/MIDS_capstone/data/nlp_by_company\n",
      "['COL.csv', 'CRM.csv', 'DGX.csv', 'FOX.csv', 'FOXA.csv', 'FTI.csv', 'JWN.csv', 'KORS.csv', 'LUV.csv', 'M.csv', 'MA.csv', 'MAA.csv', 'MAC.csv', 'MAR.csv', 'MAS.csv', 'MAT.csv', 'MCD.csv', 'MCHP.csv', 'MCK.csv', 'MCO.csv', 'MDLZ.csv', 'MDT.csv', 'MET.csv', 'MGM.csv', 'MHK.csv', 'MKC.csv', 'MLM.csv', 'MMC.csv', 'MNST.csv', 'MON.csv', 'MOS.csv', 'MPC.csv', 'MRK.csv', 'MRO.csv', 'MS.csv', 'MSFT.csv', 'MSI.csv', 'MTD.csv', 'MU.csv', 'MYL.csv', 'NAVI.csv', 'NDAQ.csv', 'NEE.csv', 'NEM.csv', 'NFLX.csv', 'NFX.csv', 'NI.csv', 'NKE.csv', 'NLSN.csv', 'NOC.csv', 'NOV.csv', 'NRG.csv', 'NSC.csv', 'NTAP.csv', 'NTRS.csv', 'NUE.csv', 'NVDA.csv', 'NWS.csv', 'NWSA.csv', 'O.csv', 'OKE.csv', 'OMC.csv', 'ORCL.csv', 'ORLY.csv', 'OXY.csv', 'PAYX.csv', 'PBCT.csv', 'PCAR.csv', 'PCG.csv', 'PCLN.csv', 'PDCO.csv', 'PEG.csv', 'PEP.csv', 'PFE.csv', 'PFG.csv', 'PG.csv', 'PGR.csv', 'PH.csv', 'PHM.csv', 'PKG.csv', 'PKI.csv', 'PLD.csv', 'PM.csv', 'PNC.csv', 'PNR.csv', 'PNW.csv', 'PPG.csv', 'PPL.csv', 'PRGO.csv', 'PRU.csv', 'PSA.csv', 'PSX.csv', 'PVH.csv', 'PWR.csv', 'PX.csv', 'PXD.csv', 'PYPL.csv', 'Q.csv', 'QCOM.csv', 'QRVO.csv', 'RCL.csv', 'REG.csv', 'REGN.csv', 'RF.csv', 'RHI.csv', 'RHT.csv', 'RJF.csv', 'RL.csv', 'RMD.csv', 'ROK.csv', 'ROP.csv', 'ROST.csv', 'RRC.csv', 'RSG.csv', 'RTN.csv', 'SBAC.csv', 'SBUX.csv', 'SCG.csv', 'SEE.csv', 'SHW.csv', 'SIG.csv', 'SLB.csv', 'SLG.csv', 'SNA.csv', 'SNI.csv', 'SNPS.csv', 'SO.csv', 'SPG.csv', 'SPGI.csv', 'SRCL.csv', 'SRE.csv', 'STI.csv', 'STT.csv', 'STX.csv', 'SWK.csv', 'SWKS.csv', 'SYF.csv', 'SYK.csv', 'SYMC.csv', 'SYY.csv', 'TAP.csv', 'TDG.csv', 'TEL.csv', 'TGT.csv', 'TIF.csv', 'TJX.csv', 'TMK.csv', 'TMO.csv', 'TRIP.csv', 'TROW.csv', 'TRV.csv', 'TSCO.csv', 'TSN.csv', 'TSS.csv', 'TWX.csv', 'TXN.csv', 'TXT.csv', 'UDR.csv', 'ULTA.csv', 'USB.csv']\n",
      "['COL', 'CRM', 'DGX', 'FOX', 'FOXA', 'FTI', 'JWN', 'KORS', 'LUV', 'M', 'MA', 'MAA', 'MAC', 'MAR', 'MAS', 'MAT', 'MCD', 'MCHP', 'MCK', 'MCO', 'MDLZ', 'MDT', 'MET', 'MGM', 'MHK', 'MKC', 'MLM', 'MMC', 'MNST', 'MON', 'MOS', 'MPC', 'MRK', 'MRO', 'MS', 'MSFT', 'MSI', 'MTD', 'MU', 'MYL', 'NAVI', 'NDAQ', 'NEE', 'NEM', 'NFLX', 'NFX', 'NI', 'NKE', 'NLSN', 'NOC', 'NOV', 'NRG', 'NSC', 'NTAP', 'NTRS', 'NUE', 'NVDA', 'NWS', 'NWSA', 'O', 'OKE', 'OMC', 'ORCL', 'ORLY', 'OXY', 'PAYX', 'PBCT', 'PCAR', 'PCG', 'PCLN', 'PDCO', 'PEG', 'PEP', 'PFE', 'PFG', 'PG', 'PGR', 'PH', 'PHM', 'PKG', 'PKI', 'PLD', 'PM', 'PNC', 'PNR', 'PNW', 'PPG', 'PPL', 'PRGO', 'PRU', 'PSA', 'PSX', 'PVH', 'PWR', 'PX', 'PXD', 'PYPL', 'Q', 'QCOM', 'QRVO', 'RCL', 'REG', 'REGN', 'RF', 'RHI', 'RHT', 'RJF', 'RL', 'RMD', 'ROK', 'ROP', 'ROST', 'RRC', 'RSG', 'RTN', 'SBAC', 'SBUX', 'SCG', 'SEE', 'SHW', 'SIG', 'SLB', 'SLG', 'SNA', 'SNI', 'SNPS', 'SO', 'SPG', 'SPGI', 'SRCL', 'SRE', 'STI', 'STT', 'STX', 'SWK', 'SWKS', 'SYF', 'SYK', 'SYMC', 'SYY', 'TAP', 'TDG', 'TEL', 'TGT', 'TIF', 'TJX', 'TMK', 'TMO', 'TRIP', 'TROW', 'TRV', 'TSCO', 'TSN', 'TSS', 'TWX', 'TXN', 'TXT', 'UDR', 'ULTA', 'USB']\n",
      "/Users/NatarajanShankar/UC_Berkeley/Final_term/Capstone/W210/MIDS_capstone/metadata\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "%cd ../data/nlp_by_company\n",
    "import glob\n",
    "xx = list(glob.glob(\"*.csv\"))\n",
    "print(xx)\n",
    "yy = []\n",
    "\n",
    "for item in xx:\n",
    "    yy.append(str(item).strip('.csv'))\n",
    "\n",
    "print(yy)\n",
    "%cd ../../metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n"
     ]
    }
   ],
   "source": [
    "company = ['COL', 'CRM', 'DGX', 'FOX', 'FOXA', 'FTI', 'JWN', 'KORS', 'LUV', 'M', 'MA', 'MAA', \n",
    "               'MAC', 'MAR', 'MAS', 'MAT', 'MCD', 'MCHP', 'MCK', 'MCO', 'MDLZ', 'MDT', 'MET', 'MGM', \n",
    "               'MHK', 'MKC', 'MLM', 'MMC', 'MNST', 'MON', 'MOS', 'MPC', 'MRK', 'MRO', 'MS', 'MSFT', \n",
    "               'MSI', 'MTD', 'MU', 'MYL', 'NAVI', 'NDAQ', 'NEE', 'NEM', 'NFLX', 'NFX', 'NI', 'NKE', \n",
    "               'NLSN', 'NOC', 'NOV', 'NRG', 'NSC', 'NTAP', 'NTRS', 'NUE', 'NVDA', 'NWS', 'NWSA', 'O', \n",
    "               'OKE', 'OMC', 'ORCL', 'ORLY', 'OXY', 'PAYX', 'PBCT', 'PCAR', 'PCG', 'PCLN', 'PDCO', \n",
    "               'PEG', 'PEP', 'PFE', 'PFG', 'PG', 'PGR', 'PH', 'PHM', 'PKG', 'PKI', 'PLD', 'PM', 'PNC', \n",
    "               'PNR', 'PNW', 'PPG', 'PPL', 'PRGO', 'PRU', 'PSA', 'PSX', 'PVH', 'PWR', 'PX', 'PXD', \n",
    "               'PYPL', 'Q', 'QCOM', 'QRVO', 'RCL', 'REG', 'REGN', 'RF', 'RHI', 'RHT', 'RJF', 'RL', \n",
    "               'RMD', 'ROK', 'ROP', 'ROST', 'RRC', 'RSG', 'RTN', 'SBAC', 'SBUX', 'SCG', 'SEE', 'SHW', \n",
    "               'SIG', 'SLB', 'SLG', 'SNA', 'SNI', 'SNPS', 'SO', 'SPG', 'SPGI', 'SRCL', 'SRE', 'STI', \n",
    "               'STT', 'STX', 'SWK', 'SWKS', 'SYF', 'SYK', 'SYMC', 'SYY', 'TAP', 'TDG', 'TEL', 'TGT', \n",
    "               'TIF', 'TJX', 'TMK', 'TMO', 'TRIP', 'TROW', 'TRV', 'TSCO', 'TSN', 'TSS', 'TWX', 'TXN', \n",
    "               'TXT', 'UDR', 'ULTA', 'USB']\n",
    "\n",
    "print(len(company))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_financial_data():\n",
    "    SandP500 = {}\n",
    "    companyList = []\n",
    "    with open(\"SandP500_symbols.txt\", \"r\") as fr:\n",
    "            for line in fr:\n",
    "                    company = json.loads(line)\n",
    "                    SandP500[company[\"symbol\"]] = line\n",
    "                    companyList.append(company[\"symbol\"])\n",
    "\n",
    "    tickerchunks = [companyList[x:x+95] for x in xrange(0, len(companyList), 95)]\n",
    "\n",
    "\n",
    "    nlp_companies = ['COL', 'CRM', 'DGX', 'FOX', 'FOXA', 'FTI', 'JWN', 'KORS', 'LUV', 'M', 'MA', 'MAA', \n",
    "               'MAC', 'MAR', 'MAS', 'MAT', 'MCD', 'MCHP', 'MCK', 'MCO', 'MDLZ', 'MDT', 'MET', 'MGM', \n",
    "               'MHK', 'MKC', 'MLM', 'MMC', 'MNST', 'MON', 'MOS', 'MPC', 'MRK', 'MRO', 'MS', 'MSFT', \n",
    "               'MSI', 'MTD', 'MU', 'MYL', 'NAVI', 'NDAQ', 'NEE', 'NEM', 'NFLX', 'NFX', 'NI', 'NKE', \n",
    "               'NLSN', 'NOC', 'NOV', 'NRG', 'NSC', 'NTAP', 'NTRS', 'NUE', 'NVDA', 'NWS', 'NWSA', 'O', \n",
    "               'OKE', 'OMC', 'ORCL', 'ORLY', 'OXY', 'PAYX', 'PBCT', 'PCAR', 'PCG', 'PCLN', 'PDCO', \n",
    "               'PEG', 'PEP', 'PFE', 'PFG', 'PG', 'PGR', 'PH', 'PHM', 'PKG', 'PKI', 'PLD', 'PM', 'PNC', \n",
    "               'PNR', 'PNW', 'PPG', 'PPL', 'PRGO', 'PRU', 'PSA', 'PSX', 'PVH', 'PWR', 'PX', 'PXD', \n",
    "               'PYPL', 'Q', 'QCOM', 'QRVO', 'RCL', 'REG', 'REGN', 'RF', 'RHI', 'RHT', 'RJF', 'RL', \n",
    "               'RMD', 'ROK', 'ROP', 'ROST', 'RRC', 'RSG', 'RTN', 'SBAC', 'SBUX', 'SCG', 'SEE', 'SHW', \n",
    "               'SIG', 'SLB', 'SLG', 'SNA', 'SNI', 'SNPS', 'SO', 'SPG', 'SPGI', 'SRCL', 'SRE', 'STI', \n",
    "               'STT', 'STX', 'SWK', 'SWKS', 'SYF', 'SYK', 'SYMC', 'SYY', 'TAP', 'TDG', 'TEL', 'TGT', \n",
    "               'TIF', 'TJX', 'TMK', 'TMO', 'TRIP', 'TROW', 'TRV', 'TSCO', 'TSN', 'TSS', 'TWX', 'TXN', \n",
    "               'TXT', 'UDR', 'ULTA', 'USB']\n",
    "    years = [2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017]\n",
    "    quarters = [\"Q1\", \"Q2\", \"Q3\", \"Q4\", \"FY\"]\n",
    "    #years = [2010]\n",
    "    #quarters = [\"Q1\", \"Q2\"]\n",
    "    #companies = [\"GE\", \"CSCO\", \"GOOG\", \"FACE\"]\n",
    "\n",
    "    bigDict = {}\n",
    "\n",
    "    attributes = [\"ticker\", \"year\", \"quarter\", \"basicdilutedeps\", \"basiceps\",\n",
    "              'cashdividendspershare', 'dilutedeps', 'incometaxexpense', 'netincome', 'netincomecontinuing',\n",
    "              'netincomediscontinued', 'netincometocommon', 'netincometononcontrollinginterest',  \n",
    "                  'operatingcostofrevenue', 'operatingrevenue',\n",
    "              'othercostofrevenue', 'otherincome', 'preferreddividends', 'sgaexpense', 'totalcostofrevenue',\n",
    "              'totalgrossprofit', 'totalinterestexpense', 'totaloperatingexpenses', 'totaloperatingincome', 'totalotherincome', \n",
    "              'totalpretaxincome', 'totalrevenue', 'weightedavebasicdilutedsharesos', 'weightedavebasicsharesos', 'weightedavedilutedsharesos'\n",
    "                 ]\n",
    "    \n",
    "\n",
    "    print(len(attributes))\n",
    "\n",
    "    xx = \",\".join(attributes) + \"\\n\"\n",
    "\n",
    "    # tickerchunks[1], tickerchunks[2], tickerchunks[3] and tickerchunks[5] are done\n",
    "    # \n",
    "    loopIndex = 0\n",
    "    #for company in tickerchunks[3]:\n",
    "    for company in nlp_companies:\n",
    "        with open(\"../data/nlp_by_company/revenue/\"+company+\"_Financials_by_Quarter.csv\", 'w') as fw:\n",
    "            fw.write(xx)\n",
    "            print(\"working on {} - {}\".format(company, loopIndex))\n",
    "#             if loopIndex > 2:\n",
    "#                 break\n",
    "            loopIndex += 1\n",
    "            #print(\"working on company {}\".format(company))\n",
    "            for year in years:\n",
    "\n",
    "                #print(\"working on year {}\".format(year))\n",
    "                for quarter in quarters:\n",
    "                    #print(\"working on quarter {}\".format(quarter))\n",
    "\n",
    "                    data, _ = updated_intrinio_get_company_financials(company, str(year), quarter)\n",
    "                    \n",
    "                    # Convert list to string\n",
    "                    datastring = \",\".join(data)\n",
    "                    \n",
    "                    # Add a linefeed so that every data point is on a different line\n",
    "                    datastring +=\"\\n\"\n",
    "\n",
    "                    # Write the data to the open file for this company\n",
    "                    fw.write(datastring)\n",
    "\n",
    "            print(\"DONE with {}\".format(company))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/NatarajanShankar/UC_Berkeley/Final_term/Capstone/W210/MIDS_capstone/metadata\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "working on COL - 0\n",
      "DONE with COL\n",
      "working on CRM - 1\n",
      "DONE with CRM\n",
      "working on DGX - 2\n",
      "DONE with DGX\n",
      "working on FOX - 3\n",
      "DONE with FOX\n",
      "working on FOXA - 4\n",
      "DONE with FOXA\n",
      "working on FTI - 5\n",
      "DONE with FTI\n",
      "working on JWN - 6\n",
      "DONE with JWN\n",
      "working on KORS - 7\n",
      "DONE with KORS\n",
      "working on LUV - 8\n",
      "DONE with LUV\n",
      "working on M - 9\n",
      "DONE with M\n",
      "working on MA - 10\n",
      "DONE with MA\n",
      "working on MAA - 11\n",
      "DONE with MAA\n",
      "working on MAC - 12\n",
      "DONE with MAC\n",
      "working on MAR - 13\n",
      "DONE with MAR\n",
      "working on MAS - 14\n",
      "DONE with MAS\n",
      "working on MAT - 15\n",
      "DONE with MAT\n",
      "working on MCD - 16\n",
      "DONE with MCD\n",
      "working on MCHP - 17\n",
      "DONE with MCHP\n",
      "working on MCK - 18\n",
      "DONE with MCK\n",
      "working on MCO - 19\n",
      "DONE with MCO\n",
      "working on MDLZ - 20\n",
      "DONE with MDLZ\n",
      "working on MDT - 21\n",
      "DONE with MDT\n",
      "working on MET - 22\n",
      "DONE with MET\n",
      "working on MGM - 23\n",
      "DONE with MGM\n",
      "working on MHK - 24\n",
      "DONE with MHK\n",
      "working on MKC - 25\n",
      "DONE with MKC\n",
      "working on MLM - 26\n",
      "DONE with MLM\n",
      "working on MMC - 27\n",
      "DONE with MMC\n",
      "working on MNST - 28\n",
      "DONE with MNST\n",
      "working on MON - 29\n",
      "DONE with MON\n",
      "working on MOS - 30\n",
      "DONE with MOS\n",
      "working on MPC - 31\n",
      "DONE with MPC\n",
      "working on MRK - 32\n",
      "DONE with MRK\n",
      "working on MRO - 33\n",
      "DONE with MRO\n",
      "working on MS - 34\n",
      "DONE with MS\n",
      "working on MSFT - 35\n",
      "DONE with MSFT\n",
      "working on MSI - 36\n",
      "DONE with MSI\n",
      "working on MTD - 37\n",
      "DONE with MTD\n",
      "working on MU - 38\n",
      "DONE with MU\n",
      "working on MYL - 39\n",
      "DONE with MYL\n",
      "working on NAVI - 40\n",
      "DONE with NAVI\n",
      "working on NDAQ - 41\n",
      "DONE with NDAQ\n",
      "working on NEE - 42\n",
      "DONE with NEE\n",
      "working on NEM - 43\n",
      "DONE with NEM\n",
      "working on NFLX - 44\n",
      "DONE with NFLX\n",
      "working on NFX - 45\n",
      "DONE with NFX\n",
      "working on NI - 46\n",
      "DONE with NI\n",
      "working on NKE - 47\n",
      "DONE with NKE\n",
      "working on NLSN - 48\n",
      "DONE with NLSN\n",
      "working on NOC - 49\n",
      "DONE with NOC\n",
      "working on NOV - 50\n",
      "DONE with NOV\n",
      "working on NRG - 51\n",
      "DONE with NRG\n",
      "working on NSC - 52\n",
      "DONE with NSC\n",
      "working on NTAP - 53\n",
      "DONE with NTAP\n",
      "working on NTRS - 54\n",
      "DONE with NTRS\n",
      "working on NUE - 55\n",
      "DONE with NUE\n",
      "working on NVDA - 56\n",
      "DONE with NVDA\n",
      "working on NWS - 57\n",
      "DONE with NWS\n",
      "working on NWSA - 58\n",
      "DONE with NWSA\n",
      "working on O - 59\n",
      "DONE with O\n",
      "working on OKE - 60\n",
      "DONE with OKE\n",
      "working on OMC - 61\n",
      "DONE with OMC\n",
      "working on ORCL - 62\n",
      "DONE with ORCL\n",
      "working on ORLY - 63\n",
      "DONE with ORLY\n",
      "working on OXY - 64\n",
      "DONE with OXY\n",
      "working on PAYX - 65\n",
      "DONE with PAYX\n",
      "working on PBCT - 66\n",
      "DONE with PBCT\n",
      "working on PCAR - 67\n",
      "DONE with PCAR\n",
      "working on PCG - 68\n",
      "DONE with PCG\n",
      "working on PCLN - 69\n",
      "DONE with PCLN\n",
      "working on PDCO - 70\n",
      "DONE with PDCO\n",
      "working on PEG - 71\n",
      "DONE with PEG\n",
      "working on PEP - 72\n",
      "DONE with PEP\n",
      "working on PFE - 73\n",
      "DONE with PFE\n",
      "working on PFG - 74\n",
      "DONE with PFG\n",
      "working on PG - 75\n",
      "DONE with PG\n",
      "working on PGR - 76\n",
      "DONE with PGR\n",
      "working on PH - 77\n",
      "DONE with PH\n",
      "working on PHM - 78\n",
      "DONE with PHM\n",
      "working on PKG - 79\n",
      "DONE with PKG\n",
      "working on PKI - 80\n",
      "DONE with PKI\n",
      "working on PLD - 81\n",
      "DONE with PLD\n",
      "working on PM - 82\n",
      "DONE with PM\n",
      "working on PNC - 83\n",
      "DONE with PNC\n",
      "working on PNR - 84\n",
      "DONE with PNR\n",
      "working on PNW - 85\n",
      "DONE with PNW\n",
      "working on PPG - 86\n",
      "DONE with PPG\n",
      "working on PPL - 87\n",
      "DONE with PPL\n",
      "working on PRGO - 88\n",
      "DONE with PRGO\n",
      "working on PRU - 89\n",
      "DONE with PRU\n",
      "working on PSA - 90\n",
      "DONE with PSA\n",
      "working on PSX - 91\n",
      "DONE with PSX\n",
      "working on PVH - 92\n",
      "DONE with PVH\n",
      "working on PWR - 93\n",
      "DONE with PWR\n",
      "working on PX - 94\n",
      "DONE with PX\n",
      "working on PXD - 95\n",
      "DONE with PXD\n",
      "working on PYPL - 96\n",
      "DONE with PYPL\n",
      "working on Q - 97\n",
      "DONE with Q\n",
      "working on QCOM - 98\n",
      "DONE with QCOM\n",
      "working on QRVO - 99\n",
      "DONE with QRVO\n",
      "working on RCL - 100\n",
      "DONE with RCL\n",
      "working on REG - 101\n",
      "DONE with REG\n",
      "working on REGN - 102\n",
      "DONE with REGN\n",
      "working on RF - 103\n",
      "DONE with RF\n",
      "working on RHI - 104\n",
      "DONE with RHI\n",
      "working on RHT - 105\n",
      "DONE with RHT\n",
      "working on RJF - 106\n",
      "DONE with RJF\n",
      "working on RL - 107\n",
      "DONE with RL\n",
      "working on RMD - 108\n",
      "DONE with RMD\n",
      "working on ROK - 109\n",
      "DONE with ROK\n",
      "working on ROP - 110\n",
      "DONE with ROP\n",
      "working on ROST - 111\n",
      "DONE with ROST\n",
      "working on RRC - 112\n",
      "DONE with RRC\n",
      "working on RSG - 113\n",
      "DONE with RSG\n",
      "working on RTN - 114\n",
      "DONE with RTN\n",
      "working on SBAC - 115\n",
      "DONE with SBAC\n",
      "working on SBUX - 116\n",
      "DONE with SBUX\n",
      "working on SCG - 117\n",
      "DONE with SCG\n",
      "working on SEE - 118\n",
      "DONE with SEE\n",
      "working on SHW - 119\n",
      "DONE with SHW\n",
      "working on SIG - 120\n",
      "DONE with SIG\n",
      "working on SLB - 121\n",
      "DONE with SLB\n",
      "working on SLG - 122\n",
      "DONE with SLG\n",
      "working on SNA - 123\n",
      "DONE with SNA\n",
      "working on SNI - 124\n",
      "DONE with SNI\n",
      "working on SNPS - 125\n",
      "DONE with SNPS\n",
      "working on SO - 126\n",
      "DONE with SO\n",
      "working on SPG - 127\n",
      "DONE with SPG\n",
      "working on SPGI - 128\n",
      "DONE with SPGI\n",
      "working on SRCL - 129\n",
      "DONE with SRCL\n",
      "working on SRE - 130\n",
      "DONE with SRE\n",
      "working on STI - 131\n",
      "DONE with STI\n",
      "working on STT - 132\n",
      "DONE with STT\n",
      "working on STX - 133\n",
      "DONE with STX\n",
      "working on SWK - 134\n",
      "DONE with SWK\n",
      "working on SWKS - 135\n",
      "DONE with SWKS\n",
      "working on SYF - 136\n",
      "DONE with SYF\n",
      "working on SYK - 137\n",
      "DONE with SYK\n",
      "working on SYMC - 138\n",
      "DONE with SYMC\n",
      "working on SYY - 139\n",
      "DONE with SYY\n",
      "working on TAP - 140\n",
      "DONE with TAP\n",
      "working on TDG - 141\n",
      "DONE with TDG\n",
      "working on TEL - 142\n",
      "DONE with TEL\n",
      "working on TGT - 143\n",
      "DONE with TGT\n",
      "working on TIF - 144\n",
      "DONE with TIF\n",
      "working on TJX - 145\n",
      "DONE with TJX\n",
      "working on TMK - 146\n",
      "DONE with TMK\n",
      "working on TMO - 147\n",
      "DONE with TMO\n",
      "working on TRIP - 148\n",
      "DONE with TRIP\n",
      "working on TROW - 149\n",
      "DONE with TROW\n",
      "working on TRV - 150\n",
      "DONE with TRV\n",
      "working on TSCO - 151\n",
      "DONE with TSCO\n",
      "working on TSN - 152\n",
      "DONE with TSN\n",
      "working on TSS - 153\n",
      "DONE with TSS\n",
      "working on TWX - 154\n",
      "API query limit reached\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-7fcc233d0b1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_financial_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-59-838815b887f6>\u001b[0m in \u001b[0;36mgenerate_financial_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m                     \u001b[0;31m#print(\"working on quarter {}\".format(quarter))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdated_intrinio_get_company_financials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompany\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquarter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0;31m# Convert list to string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "generate_financial_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/NatarajanShankar/UC_Berkeley/Final_term/Capstone/W210/MIDS_capstone/metadata\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/NatarajanShankar/UC_Berkeley/Final_term/Capstone/W210/MIDS_capstone/metadata\")\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import ast\n",
    "\n",
    "def convert_json_to_csv():\n",
    "    companyDict = {}\n",
    "    os.chdir(\"./data\")\n",
    "    #print(\"got here\")\n",
    "    for datafile in glob.glob(\"*Financials*.json\"):\n",
    "        #print(datafile)\n",
    "        with open(datafile, \"r\") as fr:\n",
    "            #print (\"Processing {}\".format(datafile))\n",
    "            for line in fr:\n",
    "                companyDict = ast.literal_eval(line)\n",
    "                \n",
    "                # Save company ticker\n",
    "                for company, companyvalue in companyDict.items():\n",
    "                    #print(\"Company is\")\n",
    "                    #print(\"{}\".format(company, end=''))\n",
    "                    # For every year subset\n",
    "                    if isinstance(companyvalue, dict):\n",
    "                        for year, yearvalue in companyvalue.items():\n",
    "                            #print(\"Year is\")\n",
    "                            #print(\"{}\".format(year, end=''))\n",
    "                            # Look at the corresponding quarter\n",
    "                            if isinstance(yearvalue, dict):\n",
    "                                datalist=[]\n",
    "                                # Look at company quarter\n",
    "                                for quarter, quartervalue in yearvalue.items():\n",
    "                                    #print(\"Quarter is\")\n",
    "                                    #print(\"{}\".format(quarter, end=''))\n",
    "                                    for financialdata in quartervalue.values():\n",
    "                                        datalist.append(financialdata)\n",
    "                                    \n",
    "                                    if len(datalist) > 21:\n",
    "                                        datalist[14] = \"IGNORE\"\n",
    "                                        datalist[21] = \"IGNORE\"\n",
    "                                    \n",
    "                                    if len(datalist) == 2:\n",
    "                                        datalist[0] = \"\"\n",
    "                                        datalist[1] = \"\"\n",
    "                                    print(\"{},{},{},{}\".format(company, year, quarter, \",\".join(datalist), end=''))\n",
    "                                    \n",
    "                                    datalist = []\n",
    "        break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLIR,2010,Q1,\n",
      "FLIR,2010,Q4,\n"
     ]
    }
   ],
   "source": [
    "convert_json_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_company_financial_data_all(symbol):\n",
    "    companyDict = {}\n",
    "    with open(\"./data/Financials.json\", \"r\") as fp:\n",
    "        #companyDict = json.load(fp)\n",
    "        for line in fp:\n",
    "            companyDict = ast.literal_eval(line)\n",
    "\n",
    "        for ticker in companyDict.keys():\n",
    "            if ticker == symbol:\n",
    "                print symbol, companyDict[symbol]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_company_financial_data_all(\"GE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_company_financial_data_by_year(symbol, year):\n",
    "    companyDict = {}\n",
    "    with open(\"./data/Financials.json\", \"r\") as fp:\n",
    "        #companyDict = json.load(fp)\n",
    "        for line in fp:\n",
    "            companyDict = ast.literal_eval(line)\n",
    "\n",
    "        for ticker in companyDict.keys():\n",
    "            if ticker == symbol:\n",
    "                print symbol, companyDict[symbol][year]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_company_financial_data_by_year(\"GE\", \"2014\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_company_financial_data_by_year_and_attr(symbol, year, attribute):\n",
    "    companyDict = {}\n",
    "    with open(\"./data/Financials.json\", \"r\") as fp:\n",
    "        #companyDict = json.load(fp)\n",
    "        for line in fp:\n",
    "            companyDict = ast.literal_eval(line)\n",
    "\n",
    "        for ticker in companyDict.keys():\n",
    "            if ticker == symbol:\n",
    "                print symbol, companyDict[symbol][year][attribute]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_company_financial_data_by_year_and_attr(\"GE\", \"2014\", \"basiceps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_company_financial_data_csv(symbol):\n",
    "    companyDict = {}\n",
    "    with open(\"./data/Financials.json\", \"r\") as fp:\n",
    "        for line in fp:\n",
    "            companyDict = ast.literal_eval(line)\n",
    "            \n",
    "        for ticker in companyDict.keys():\n",
    "            if ticker == symbol:\n",
    "                print symbol, companyDict\n",
    "            \n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
