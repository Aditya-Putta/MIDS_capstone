{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime\n",
    "#import pandas.io.data as web\n",
    "from pandas_datareader import data\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]]]\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((2,3,4))\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/AAPL.csv AAPL\n",
      "../data/all_tweets.csv all_tweets\n"
     ]
    },
    {
     "ename": "CParserError",
     "evalue": "Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCParserError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-798b05848f6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcolname_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticker_idx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_ticker1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m \u001b[0mcolname_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticker_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_ticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_csv_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#/sp500\") #, days_for_prediction=30)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;31m#print(train_X, test_X)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;31m#print(len(colname_idx), train_max_rowcount, test_max_rowcount)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-798b05848f6d>\u001b[0m in \u001b[0;36mload_csv_data\u001b[0;34m(source_dir, days_for_prediction)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m#df = pd.read_csv(filename, parse_dates=[0]) #index_col='Date')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m#  Open, High , Low , Close, Adj Close, Volume\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Split 80/20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/thongbui/anaconda3/envs/py36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    644\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/thongbui/anaconda3/envs/py36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m     \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/thongbui/anaconda3/envs/py36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    937\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/thongbui/anaconda3/envs/py36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1506\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1508\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1509\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.read (pandas/parser.c:10415)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._read_low_memory (pandas/parser.c:10691)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._read_rows (pandas/parser.c:11437)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._tokenize_rows (pandas/parser.c:11308)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.raise_parser_error (pandas/parser.c:27037)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCParserError\u001b[0m: Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n"
     ]
    }
   ],
   "source": [
    "# Load csvs and create train, test datasets\n",
    "def get_ticker_idx_maps(dfs):\n",
    "    sorted_keys = sorted(dfs.keys())\n",
    "    ticker_idx = {ticker: i for i,ticker in enumerate(sorted_keys)}\n",
    "    idx_ticker = {i: ticker for i,ticker in enumerate(sorted_keys)}\n",
    "    return ticker_idx, idx_ticker\n",
    "\n",
    "\n",
    "#Create 3d array from dict entry\n",
    "def create_3d(max_row, map):\n",
    "    a = []\n",
    "    for k,v in map.items():\n",
    "        col_num = len(v[0])\n",
    "        # Pad the smaller array with nan values\n",
    "        padded_rows = max_rows - len(v)\n",
    "        if (padded_rows == 0):\n",
    "            a += [v]\n",
    "        else:\n",
    "            pad = [[np.nan]* col_num]*padded_rows #np.zeros((]padded_rows,col_num))\n",
    "            a += [pad + v]\n",
    "    return np.array(a)\n",
    "\n",
    "def pad_rows(max_rows, col_num, v):\n",
    "    padded_rows = max_rows - len(v)\n",
    "    #print('pad_rows: max_rows', max_rows,'col_num =', col_num, 'padded_rows =', padded_rows)\n",
    "#    print(v)\n",
    "    a = []\n",
    "    if (padded_rows == 0):\n",
    "        a = v\n",
    "    else:\n",
    "        # Concate the 2 matrices vertically\n",
    "        if (col_num == 1):\n",
    "            pad = [np.nan]*padded_rows\n",
    "            a = pad + list(v)\n",
    "        else:\n",
    "            pad = np.array([[np.nan]* col_num]*padded_rows)\n",
    "            #print(pad.shape, v.shape)\n",
    "            #a = np.column_stack((pad,v))\n",
    "            a = np.concatenate((pad,v),axis=0)\n",
    "\n",
    "    return np.array(a)\n",
    "\n",
    "\n",
    "def dict_2d_to_3d(dfs, ticker_idx, colname_idx, shift_value):\n",
    "    sorted_keys = sorted(dfs.keys())\n",
    "    # dfs with tickers in keys -> dfs with keys as index(tickers)\n",
    "    dfs_int_keys = {ticker_idx[ticker]: dfs[ticker].values for ticker in sorted_keys}\n",
    "    # then convert it to 3d array\n",
    "    X = np.array(list(np.array(pd.DataFrame.from_dict(dfs_int_keys[key])) for key in sorted(dfs_int_keys.keys())))\n",
    "    y = np.array(list(dfs_int_keys[key][:,colname_idx['Adj Close']] for key in sorted(dfs_int_keys.keys())))\n",
    "\n",
    "    #print(\"before shifting: y[0][:\", shift_value, '] =', y[0][:shift_value])\n",
    "    #print(\"before shifting: y[0][-\", shift_value, ':] =', y[0][-shift_value:])\n",
    "    print(\"dict_2d_to_3d: X.shape =\", X.shape, \"y.shape=\", y.shape)\n",
    "\n",
    "    max_row_count = 0\n",
    "    X2 = None\n",
    "    y2 = None\n",
    "    for i in range(len(y)):\n",
    "        y[i] = np.roll(y[i], -shift_value, axis=0)\n",
    "        # The last shift_value rows have no data\n",
    "        #y[i][-shift_value:] = None\n",
    "        \n",
    "        # Remove the last shift_value elements of the arrays\n",
    "        #y[i] = y[i][:len(y[i])-shift_value].copy()\n",
    "        #X[i] = X[i][:len(X[i])-shift_value].copy()\n",
    "        yi = y[i][:len(y[i])-shift_value].copy()\n",
    "        Xi = X[i][:len(X[i])-shift_value].copy()\n",
    "        rowcount = len(Xi)\n",
    "        if (rowcount > max_row_count):\n",
    "            max_row_count = rowcount\n",
    "        #print(max_row_count)\n",
    "        \n",
    "        yi_res = pad_rows(max_row_count, 1, yi)\n",
    "        Xi_res = pad_rows(max_row_count, X[0].shape[1], Xi)\n",
    "\n",
    "        if (X2 is None):\n",
    "            y2 = np.array([yi_res]).reshape(len(yi_res),1)\n",
    "            X2 = np.array([Xi_res]).reshape(Xi_res.shape[0], Xi_res.shape[1])\n",
    "        else:\n",
    "            y2 = np.concatenate((y2, np.array([yi_res]).reshape(len(yi_res),1)), axis=0)\n",
    "            X2 = np.concatenate((X2, Xi_res), axis=0)\n",
    "\n",
    "    # 2d -> 3d\n",
    "    X2 = X2.reshape(len(y), Xi_res.shape[0], Xi_res.shape[1])\n",
    "    y2 = y2.reshape(len(y), Xi_res.shape[0], 1)    \n",
    "    return X2, y2\n",
    "\n",
    "\n",
    "def dfs_to_3d(dfs, ticker_idx, colname_idx, shift_value):\n",
    "    sorted_keys = sorted(dfs.keys())\n",
    "    # dfs with tickers in keys -> dfs with keys as index(tickers)\n",
    "    dfs_int_keys = {ticker_idx[ticker]: dfs[ticker].values for ticker in sorted_keys}\n",
    "    # then convert it to 3d array\n",
    "    X = np.array(list(np.array(pd.DataFrame.from_dict(dfs_int_keys[key])) for key in sorted(dfs_int_keys.keys())))\n",
    "    y = np.array(list(dfs_int_keys[key][:,colname_idx['Adj Close']] for key in sorted(dfs_int_keys.keys())))\n",
    "    print(\"dfs_to_3d: X.shape =\", X.shape, \"y.shape=\", y.shape)\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        y[i] = np.roll(y[i], -shift_value, axis=0)\n",
    "        # Remove the last shift_value elements of the arrays\n",
    "        y[i] = np.array(y[i][:len(y[i])-shift_value]) #.copy()\n",
    "        X[i] = np.array(X[i][:len(X[i])-shift_value]) #.copy()\n",
    "        \n",
    "    return X, y #np.dstack(temp)\n",
    "\n",
    "\n",
    "def load_csv_data(source_dir=\"../data\", days_for_prediction=30):\n",
    "    '''\n",
    "    Input:\n",
    "    - source_dir: directory where the stock price CSVs are located\n",
    "    - days_for_prediction: number of days for the prediction prices. Must be at least 30 days\n",
    "    Description:\n",
    "    Read csv files in source_dir, load into dataframes and split into\n",
    "    X_train, Y_train, X_test, Y_test\n",
    "    '''\n",
    "    assert (days_for_prediction >= 30), \"days_for_prediction must be >= 30\"\n",
    "\n",
    "    csv_file_pattern = os.path.join(source_dir, \"*.csv\")\n",
    "    csv_files = glob.glob(csv_file_pattern)\n",
    "    dfs1 = {}\n",
    "    dfs2 = {}\n",
    "    for filename in csv_files:\n",
    "        arr = filename.split('/')\n",
    "        ticker = arr[-1].split('.')[0]\n",
    "        print(filename, ticker)\n",
    "        #print(ticker, df.head())        \n",
    "        #  Date, Open, High , Low , Close, Adj Close, Volume\n",
    "        #df = pd.read_csv(filename, parse_dates=[0]) #index_col='Date')       \n",
    "        #  Open, High , Low , Close, Adj Close, Volume\n",
    "        df = pd.read_csv(filename, index_col='Date')\n",
    "\n",
    "        # Split 80/20\n",
    "        split_len = int(len(df) * .8)\n",
    "        #print(split_len)\n",
    "        df1 = df.iloc[:split_len,:]\n",
    "        df2 = df.iloc[split_len:,:]\n",
    "\n",
    "        #print(\"df1.tail\", df1.tail())\n",
    "        #print(\"df2.head\", df2.head())\n",
    "        dfs1[ticker] = df1\n",
    "        dfs2[ticker] = df2\n",
    "\n",
    "        #print(len(df), len(df1)/len(df), len(df2)/len(df))\n",
    "    #print(max_row_count1, max_row_count2)\n",
    "    \n",
    "    colname_idx = {colname: i for i, colname in enumerate(list(dfs1[ticker]))}\n",
    "    ticker_idx1, idx_ticker1 = get_ticker_idx_maps(dfs1)\n",
    "    ticker_idx2, idx_ticker2 = get_ticker_idx_maps(dfs2)\n",
    "\n",
    "    # Validate that the 2 mapping sets are the same for train and test data\n",
    "    shared_items = set(ticker_idx1.items()) & set(ticker_idx2.items())\n",
    "    assert(len(shared_items) == len(ticker_idx1) and len(shared_items) == len(ticker_idx2))\n",
    "    \n",
    "    # Convert dict of 2d arrays to 3d arrays\n",
    "    #train_X, train_y = dict_2d_to_3d(dfs1, ticker_idx1, colname_idx, days_for_prediction)\n",
    "    #test_X, test_y = dict_2d_to_3d(dfs2, ticker_idx2, colname_idx, days_for_prediction)\n",
    "    train_X, train_y = dfs_to_3d(dfs1, ticker_idx1, colname_idx, days_for_prediction)\n",
    "    test_X, test_y = dfs_to_3d(dfs2, ticker_idx2, colname_idx, days_for_prediction)\n",
    "\n",
    "\n",
    "    return colname_idx, ticker_idx1, idx_ticker1, train_X, train_y, test_X, test_y\n",
    "\n",
    "colname_idx, ticker_idx, idx_ticker, train_X, train_y, test_X, test_y = load_csv_data(\"../data\") #/sp500\") #, days_for_prediction=30)\n",
    "#print(train_X, test_X)\n",
    "#print(len(colname_idx), train_max_rowcount, test_max_rowcount)\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "#print(train_X[0], test_X[0])\n",
    "print(train_X[0].shape, train_X[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(train_y[0]), train_y[0])\n",
    "print(len(train_y[1]), train_y[1])\n",
    "print(colname_idx['Adj Close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print((train_X[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check shape for these 501 stocks\n",
    "# Shape is [number_of_stocks, number_price_entries, number_of_features]\n",
    "for i in range(len(train_X)):\n",
    "    print(i, idx_ticker[i], 'train:', train_X[i].shape, train_y[i].shape,'test:', test_X[i].shape, test_y[i].shape) #round(train_X[i].shape[0]/(train_X[i].shape[0]+test_X[i].shape[0]),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print('train_X[0][29:39]', train_X[0][29:39])\n",
    "import math\n",
    "\n",
    "adj_close_idx = colname_idx['Adj Close']\n",
    "\n",
    "'''\n",
    "Checking to see if y price is matching with X data + 30 days\n",
    "'''\n",
    "def check(train_X, train_y, shift_value=30):\n",
    "    print('Checking X, y data ... ')\n",
    "    for k in range(len(train_X)):\n",
    "        train_yi = train_y[k]\n",
    "        train_Xi = train_X[k]\n",
    "        for i in range(len(train_yi) - shift_value):\n",
    "            x_val = train_Xi[i + shift_value, adj_close_idx]\n",
    "            #print(x_val, train_yi[i])\n",
    "            #if (not math.isnan(train_yi[i][0]) and train_yi[i][0] != x_val):\n",
    "            if (not math.isnan(train_yi[i]) and train_yi[i] != x_val):\n",
    "                print(\"error: \", k, i, train_yi[i], x_val)\n",
    "\n",
    "    print(\"Everything looks good for X and y!\")\n",
    "    \n",
    "'''\n",
    "def check2(train_Xi, train_yi):\n",
    "    print(len(train_Xi), len(train_yi))\n",
    "    print('train_Xi[30:40,adj_close_idx]', train_Xi[30:40,adj_close_idx])\n",
    "    # Should be same as above\n",
    "    y_1st_10 = train_yi[:10].reshape(10)\n",
    "    print('train_yi[:10]', y_1st_10)\n",
    "    assert(np.array_equal(train_Xi[30:40,adj_close_idx], y_1st_10))\n",
    "\n",
    "\n",
    "    print('train_Xi[-40:,adj_close_idx]', train_Xi[-40:,adj_close_idx])\n",
    "    print('train_yi[-10:]', train_yi[-10:])\n",
    "    #print(train_yi)\n",
    "'''\n",
    "\n",
    "check(train_X, train_y)\n",
    "check(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now try with baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 model w 3-d matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Activation, LSTM\n",
    "from keras.utils import plot_model\n",
    "\n",
    "max_features=len(colname_idx)\n",
    "\n",
    "window_size=30\n",
    "model = Sequential()\n",
    "# model.add(LSTM(hidden, input_shape=(examples, features)))\n",
    "model.add(LSTM(units=window_size, input_shape=(None,max_features), return_sequences=True, dropout=0.2))\n",
    "model.add(LSTM(window_size*2, return_sequences=False, dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "png_file='lstm.png'\n",
    "plot_model(model, to_file=png_file, show_shapes=True, show_layer_names=True)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=png_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "print(train_X.shape, train_y.shape)\n",
    "model.fit(x=train_X, y=train_y, batch_size=1, epochs=10, shuffle=True)\n",
    "score = model.evaluate(test_X, test_Y, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 model per stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Activation, LSTM\n",
    "from keras.utils import plot_model\n",
    "\n",
    "max_features=len(colname_idx)\n",
    "models=[]\n",
    "\n",
    "def plot_results(predicted_data, true_data):\n",
    "    fig = plt.figure(facecolor='white')\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(true_data, label='True Data')\n",
    "    plt.plot(predicted_data, label='Prediction')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def normalize_windows(window_data):\n",
    "    normalised_data = []\n",
    "    for window in window_data:\n",
    "        normalised_window = [((p / window[0]) - 1) for p in window]\n",
    "        normalised_data.append(normalised_window)\n",
    "    return normalised_data\n",
    "\n",
    "\n",
    "window_size = 30\n",
    "for i in range(len(train_X)):    \n",
    "    model = Sequential()\n",
    "    # model.add(LSTM(hidden, input_shape=(examples, features)))\n",
    "    #model.add(LSTM(max_features, input_shape=(None, train_X[i].shape[1]), dropout=0.2))\n",
    "    print(i, type(train_X[i])) #.shape)\n",
    "    model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "\n",
    "\n",
    "    #model.add(LSTM(1, input_shape=(max_features,1), return_sequences=True, dropout=0.2))\n",
    "    #model.add(LSTM(max_features, return_sequences=False, dropout=0.2))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    #model.compile(loss='mse', optimizer='rmsprop')\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "    print('model.output_shape', model.output_shape)\n",
    "\n",
    "    plot_model(model, to_file=idx_ticker[i] + '.png', show_shapes=True, show_layer_names=True)\n",
    "    print(idx_ticker[i], model.summary())\n",
    "    print('train_X[i].shape', train_X[i].shape, 'train_y[i].shape', train_y[i].shape)\n",
    "    \n",
    "    # Normalize data\n",
    "    '''\n",
    "    print(\"Normalizing data....\")\n",
    "    train_X[i] = normalize_windows(train_X[i])\n",
    "    train_y[i]= normalize_windows(train_y[i])\n",
    "    test_X[i] = normalize_windows(test_X[i])\n",
    "    test_y[i]= normalize_windows(test_y[i])\n",
    "    '''\n",
    "\n",
    "    \n",
    "    # Reshape to match with input from 1st LSTM\n",
    "    x = np.reshape(train_X[i], (train_X[i].shape[0], max_features, 1))\n",
    "    y = np.reshape(train_y[i], (train_y[i].shape[0], 1)) \n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "    model.fit(x, y, batch_size=100, epochs=5, shuffle=True)\n",
    "    # Test\n",
    "    x2 = np.reshape(test_X[i], (test_X[i].shape[0], max_features,1))\n",
    "    y2 =np.reshape(test_y[i], (test_y[i].shape[0], 1))\n",
    "    score = model.evaluate(x2, y2, batch_size=16)\n",
    "    print(\"score = \", score)\n",
    "    predicted = model.predict(x2)\n",
    "    print(\"predicted.shape\", predicted.shape)\n",
    "    plot_results(predicted, y2)\n",
    "\n",
    "    models.append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=idx_ticker[0] + '.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map = {0:[[2,3],[4,5],[6,6]], 1: [[1,1],[2,2]], 2:[[0,0],[3,4]], 3:[[0,0],[1,1]]}\n",
    "print(map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.array([v for k,v in map.items()])\n",
    "print(a)\n",
    "print(a.shape)\n",
    "a2 = np.zeros((4,3,2))\n",
    "print(a2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
